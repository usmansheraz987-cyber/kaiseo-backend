const natural = require("natural");
const tokenizer = new natural.SentenceTokenizer();
const aiClient = require("../utils/aiClient"); // your existing LLM client

function computePerplexity(text) {
  const words = text.split(/\s+/);
  return Math.min(100, Math.max(5, 100 - Math.log(words.length) * 10));
}

function computeBurstiness(text) {
  const sentences = tokenizer.tokenize(text);
  if (sentences.length < 2) return 20;

  const lengths = sentences.map(s => s.split(" ").length);
  const avg = lengths.reduce((a, b) => a + b) / lengths.length;
  const variance = lengths.reduce((a, b) => a + Math.pow(b - avg, 2), 0) / lengths.length;

  return Math.min(100, variance * 2);
}

function computeEntropy(text) {
  const chars = text.split("");
  const freq = {};
  chars.forEach(c => freq[c] = (freq[c] || 0) + 1);
  const total = chars.length;

  let entropy = 0;
  Object.values(freq).forEach(count => {
    const p = count / total;
    entropy -= p * Math.log2(p);
  });

  return Math.min(100, entropy * 10);
}

async function aiProbabilityCheck(text) {
  try {
    const prompt = `
Estimate the probability that the following text was generated by AI.
Respond only with a number 0â€“1.

Text:
${text}
`;

    const result = await aiClient.callModel(prompt);
    const num = parseFloat(result.trim());
    if (isNaN(num)) return 0.5;
    return Math.min(1, Math.max(0, num));
  } catch (e) {
    return 0.5;
  }
}

async function detectAI(text) {
  const perplexity = computePerplexity(text);
  const burstiness = computeBurstiness(text);
  const entropy = computeEntropy(text);
  const aiProb = await aiProbabilityCheck(text);

  const combined =
    (perplexity * 0.25) +
    (burstiness * 0.25) +
    (entropy * 0.25) +
    ((1 - aiProb) * 100 * 0.25);

  return {
    score: Math.round(combined),
    perplexity,
    burstiness,
    entropy,
    aiProbability: aiProb
  };
}

module.exports = detectAI;
